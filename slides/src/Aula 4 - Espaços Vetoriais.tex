\documentclass{beamer}
\usetheme[logo=brasaoUFSC.png]{fibeamer}
%% These macros specify information about the presentation
\title{Aula 4: Espaços Vetoriais}
%\subtitle{2019.2}
\author{Melissa Weber Mendonça}
%% These additional packages are used within the document:
\usepackage{ragged2e}  % `\justifying` text
\usepackage{booktabs}  % Tables
\usepackage{tabularx}
\usepackage{tikz}      % Diagrams
\usetikzlibrary{calc, shapes, backgrounds, arrows}
\usepackage{pgfplots}
\usepackage{amsmath, amssymb}
\usepackage{url}       % `\url`s
\usepackage{listings}  % Code listings
\usepackage{verbatim}
\usepackage{animate}
\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    allcolors = [rgb]{1,0.83,0.39}
}
\frenchspacing

\makeatletter
\setlength\fibeamer@lengths@logowidth{4em}
\setlength\fibeamer@lengths@logoheight{5em}
\makeatother

\newcommand{\zerodisplayskips}{%
  \setlength{\abovedisplayskip}{0pt}%
  \setlength{\belowdisplayskip}{0pt}%
  \setlength{\abovedisplayshortskip}{0pt}%
  \setlength{\belowdisplayshortskip}{0pt}}

\begin{document}
\frame{\maketitle}

\begin{darkframes}

  % \begin{frame}{Subespaços de uma matriz}
  %   Considere agora uma matriz $A\in {\mathbb{R}}^{m\times n}$. Chegamos então ao caso interessante de subespaços ligados à matriz $A$ em um sistema linear $Ax=b$, com $x\in {\mathbb{R}}^n$ e $b\in {\mathbb{R}}^m$. Para relacionarmos o conceito de subespaços vetoriais à resolução de sistemas de equações lineares, precisamos do conceito de combinação linear.
  % \end{frame}
  
\begin{frame}{Variedades afins}
  \begin{block}{Definição}
    Um subconjunto $V\subset E$ chama-se uma \emph{variedade afim} quando a reta que une dois pontos quaisquer de $V$ está contida em $V$. Assim, $V\subset E$ é uma variedade afim se e somente se cumpre a seguinte condição:
   \begin{equation*}
     x,y \in V, \, t \in {\mathbb{R}} \Rightarrow (1-t)x + ty \in V.
   \end{equation*}
 \end{block}
 \vfill
 \only<2>{\alert{Exemplo.} Todo subespaço é também uma variedade afim.}
\end{frame}

\begin{frame}{Observação}
  Se $V_1,\ldots, V_m \subset E$ são variedades afins, então a intersecção $V_1 \cap V_2\cap \ldots \cap V_m$ é ainda uma variedade afim. Todo ponto $p\in E$ é uma variedade afim.
\end{frame}

\begin{frame}{Exemplo}
  Sejam $a_1,\ldots, a_n, b$ números reais. O conjunto dos pontos $x=(x_1,\ldots,x_n)\in {\mathbb{R}}^n$ tais que 
   \begin{equation*}
     a_1x_1+\ldots+a_nx_n=b
   \end{equation*}
   é uma variedade afim, que não contém a origem quando $b \ne 0$. Se os números $a_i$ não forem todos nulos, chamamos esta veriedade ${\mathcal{H}}$ de \emph{hiperplano}. Se $a_1=\ldots=a_n=0$, então ${\mathcal{H}}=\emptyset$ quando $b\ne 0 $ e ${\mathcal{H}}={\mathbb{R}}^n$ quando $b=0$.
   \vfill
   \only<2>{Mais geralmente, \alert{o conjunto das soluções de um sistema linear de $m$ equações com $n$ incógnitas é uma variedade afim, intersecção das $m$ variedades afins definidas pelas equações do sistema.}}
\end{frame}

\begin{frame}{Combinação Linear}
    \begin{block}{Definição}
      Seja $E$ um espaço vetorial, e sejam $u_1,u_2,\ldots,u_n \in E$ vetores neste espaço. Então uma \emph{combinação linear} destes vetores é um vetor $u$ no espaço $E$ dado por
      \begin{equation*}
        u = \alpha_1 u_1+\ldots+\alpha_n u_n,
      \end{equation*}
      para $\alpha_1,\ldots,\alpha_n \in {\mathbb{K}}$.
    \end{block}
  \end{frame}
  
  \begin{frame}{Espaço Gerado: span}
    \begin{block}{Definição}
      Uma vez fixados os vetores $\{ v_1,\ldots,v_n\}$ em $V$, o conjunto $W\subset V$ que contém todas as combinações lineares destes vetores é chamado de \emph{espaço gerado} pelos vetores $v_1,\ldots,v_n$. Denotamos isto por
      \begin{align*}
        W &= \text{span}\{v_1,\ldots,v_n\}\\
          &= \{ v \in V : v = a_1v_1 + a_2v_2 + \ldots + a_n v_n, a_i\in {\mathbb{K}}, 1\leq i \leq n\}.
      \end{align*}
    \end{block}
  \end{frame}
  
  \begin{frame}{Espaço Gerado: span}
    Uma outra caracterização de subespaço gerado é a seguinte:
    \vfill
    \alert{$W$ é o menor subespaço de $V$ que contém o conjunto de vetores $\{ v_1,\ldots,v_n\}$} no sentido que qualquer outro subespaço $W'$ de $V$ que contenha estes vetores satisfará $W' \supset W$, já que como $v_1,\ldots,v_n \in W'$ e $W'$ é um subespaço vetorial, então qualquer combinação linear destes vetores também está incluida em $W'$; logo $W\subset W'$.
  \end{frame}
  
  \begin{frame}{Independência Linear}
    \begin{block}{Definição}
      Seja $X=\{v_1,v_2,\ldots,v_n\}\subset E$ um conjunto de vetores. Se nenhum dos vetores $v_i$ puder ser escrito como combinação linear dos outros vetores, dizemos que este conjunto é linearmente independente (l.i.). Formalmente, o conjunto $X$ é l.i. se e somente se a única combinação linear nula dos vetores de $X$ for aquela cujos coeficientes são todos nulos, ou seja,
      \begin{equation*}
        \alpha_1v_1+\alpha_2 v_2 + \ldots + \alpha_n v_n = 0 \Rightarrow \alpha_1=\alpha_2=\ldots=\alpha_n=0.
      \end{equation*}
      Se um conjunto $X$ de vetores em um espaço vetorial $E$ não é l.i., dizemos que ele é linearmente dependente (l.d.).
    \end{block}
    \only<2>{\alert{Evidentemente}, todo subconjunto de um conjunto l.i. é também l.i.}
  \end{frame}
  
  \begin{frame}{Exemplos}
    \only<1>{\begin{itemize}
      \item Em ${\mathbb{R}}^2$, quaisquer dois vetores que não sejam colineares são l.i. 
      \end{itemize}}
    \only<2>{\begin{itemize}
      \item Em ${\mathbb{R}}^n$, chamamos de vetores canônicos os vetores definidos como, para todos $i,j = 1,\ldots,n$
        \begin{equation*}
          (e_i)_j = \begin{cases}
            1, & \mbox{ se } j = i\\
            0, & \mbox{ caso contrário.}
          \end{cases}
        \end{equation*}
        em que o subíndice $j$ denota a coordenada $j$ do i-ésimo vetor canônico. Estes vetores são l.i.
      \end{itemize}}
    \only<3>{\begin{itemize}
      \item Em ${\mathbb{R}}^{2\times 2}$, as matrizes
        \begin{equation*}
          A = \begin{pmatrix} 1 & 0\\0 & 0\end{pmatrix} \text{ e } B = \begin{pmatrix} 0 & 1\\0 & 0\end{pmatrix}
        \end{equation*}
        são l.i.
      \end{itemize}}
    \only<4>{\begin{itemize}
      \item O conjunto ${\mathcal{P}}({\mathbb{R}})$ dos polinômios
        \begin{equation*}
          p(x) = a_0 + a_1x + \ldots + a_nx^n
        \end{equation*}
        é um subespaço de ${\mathcal{F}}({\mathbb{R}} ;{\mathbb{R}})$, assim como o conjunto ${\mathcal{P}}_n$ dos polinômios de grau $\leq n$.
        \vfill
        Qual seria um conjunto l.i. nesse subespaço?
      \end{itemize}}
\end{frame}

\begin{frame}{Exemplo}
  \alert{Note que o conjunto dos polinômios de grau $n$ não é um subespaço, pois a soma de dois polinômios de grau $n$ pode ter grau $<n$!}

  \only<2>{Os monômios $1,x,\ldots,n^n$ em ${\mathcal{P}}_n$ são l.i., pois $\alpha_0 + \alpha_1 x + \ldots + \alpha n x^n = p(x)$ é o vetor nulo em ${\mathcal{P}}_n$ somente quando $p(x)$ é o polinômio identicamente nulo, ou seja, $p(x) = 0 $ para todo $x\in {\mathbb{R}}$. Isto implica que $\alpha_0 = \ldots = \alpha_n=0$, pois um polinômio não nulo de grau $k$ tem no máximo $k$ raízes reais. Podemos, além disso, concluir que o conjunto $X = \{ 1,x,\ldots,x^n,\ldots\}\subset {\mathcal{P}}$ é um conjunto infinito l.i.}
\end{frame}

\begin{frame}{}
  \begin{block}{Teorema}
    Se $v=\alpha_1v_1+\ldots+\alpha_mv_m = \beta_1v_1+\ldots+\beta_mv_m$ e os vetores $v_1,\ldots,v_m$ são l.i., então $\alpha_1=\beta_1, \alpha_2=\beta_2, \ldots, \alpha_m=\beta_m$.
  \end{block}
\end{frame}

\begin{frame}{Base de um espaço vetorial}
  Gostaríamos de encontrar, para um espaço $W$ qualquer, um conjunto de vetores de forma que qualquer outro vetor em $W$ possa ser escrito como combinação linear destes vetores (como $i,j,k$ em ${\mathbb{R}}^3$, por exemplo)
  \begin{block}{Definição}
    Uma \emph{base} de um espaço vetorial $E$ é um conjunto ${\mathcal{B}}\subset E$ linearmente independente que gera $E$, ou seja, todo vetor $v\in E$ se exprime, de modo único, como combinação linear $v=\alpha_1 v_1 + \ldots + \alpha_m v_m$ de elementos $v_1,\ldots,v_m$ da base ${\mathcal{B}}$. Se ${\mathcal{B}} = \{ v_1,\ldots,v_m\}$ é uma base de $E$ e $v=\alpha_1 v_1 + \ldots +\alpha_mv_m$, então os números $\alpha_1,\ldots,\alpha_m$ chamam-se as \emph{coordenadas} do vetor $v$ na base ${\mathcal{B}}$.   
  \end{block}
\end{frame}

\begin{frame}{Exemplo}
  \only<1>{%
    \begin{itemize}
    \item Base canônica no ${\mathbb{R}}^n$.
    \end{itemize}
  }
  \only<2>{%
    \begin{itemize}
    \item Os monômios $1,x,\ldots,x^n$ formam uma base para o espaço vetorial ${\mathcal{P}}_n$ dos polinômios de grau $\leq n$. O conjunto
      \begin{equation*}
        \{ 1,x,\ldots,x^n,\ldots \}
      \end{equation*}
      dos monômios de graus arbitrários constitui uma base (infinita) para o espaço vetorial ${\mathcal{P}}$ de todos os polinômios reais.
    \end{itemize}
    }
\end{frame}

\begin{frame}{Resultados sobre bases}
  \begin{block}{Lema}
    Sejam $v_1,\ldots,v_n\ne 0$ que geram um e.v. $E$. Então dentre estes vetores podemos extrair uma base de $E$.
  \end{block}
  \vfill
  \only<2>{\alert{Demonstração.} Se $v_1,\ldots,v_n$ forem l.i., não há nada a fazer. Suponha então que eles sejam l.d. Então,
  \begin{equation*}
    \zerodisplayskips
    x_1v_1+\ldots+x_nv_n = 0
  \end{equation*}
  com pelo menos algum $x_i\ne 0$. Sem perda de generalidade, suponha que $x_n\ne 0$ (a ordem não importa). Então, escreva
  \begin{equation*}
    \zerodisplayskips
    v_n = -\frac{x_1}{x_n} v_i - \ldots - \frac{x_{n-1}}{x_n} v_{n-1}
  \end{equation*}
  Desta forma, $v_1, \ldots, v_{n-1}$ ainda geram $E$. Prossiga desta maneira até que todos os elementos l.d. tenham sido eliminados e teremos uma base de $E$.}
\end{frame}

% O Lema seguinte nos dá uma amostra da ligação entre os espaços vetoriais e as soluções dos sistemas lineares.

\begin{frame}{Sistemas Homogêneos}
  \begin{block}{Lema}
    Todo sistema linear homogêneo cujo número de incógnitas é maior do que o número de equações admite uma solução não-trivial.
  \end{block}
  \vfill
  \only<2>{\alert{Demonstração.} Consideremos o sistema
    \begin{equation*}
      \zerodisplayskips
      \begin{cases}
        a_{11}x_1 + a_{12}x_2 + \ldots + a_{1n}x_n &= 0\\
        a_{21}x_1 + a_{22}x_2 + \ldots + a_{2n}x_n &= 0\\
        \vdots & \\
        a_{m1}x_1 + a_{m2}x_2 + \ldots + a_{mn}x_n &= 0
      \end{cases}
    \end{equation*}
    de $m$ equações e $n$ incógnitas, onde $m<n$. Vamos provar o resultado por indução no número de equações do sistema.}
  
  \only<3>{\footnotesize{Se tivermos apenas uma equação do tipo
      \begin{equation*}
        \zerodisplayskips
        a_{11}x_1 + \ldots + a_{1n}x_n = 0
      \end{equation*}
      com $n>1$ incógnitas, devemos ter um dos dos coeficientes $a_{1i}\ne 0$ (caso contrário esta equação não faria sentido). Podemos supor então, sem perda de generalidade, que $a_{1n}\ne 0$. Isolando $x_n$ na equação dada, temos
      \begin{equation*}
        \zerodisplayskips
        x_n = - \left( \frac{a_{11}}{a_{1n}} x_1 + \ldots + \frac{a_{1,n-1}}{a_{1n}} x_{n-1}\right).
      \end{equation*}
      Para obtermos uma solução não-trivial para a equação do sistema, basta escolhermos valores quaisquer para os $x_1,\ldots,x_{n-1}$ (que são variáveis livres) e obteremos $x_n$.}}
  
  \only<4>{\footnotesize{Para completar a indução, vamos supor que o lema seja verdadeiro para um sistema com $m-1$ equações. Podemos primeiramente admitir que, no sistema original, temos $a_{mn} \ne 0$ (caso contrário, o sistema não teria $m$, mas $m-1$ equações). Então, a $m$-ésima equação pode ser reescrita como
      \begin{equation*}
        x_n = -\left( \frac{a_{m1}}{a_{mn}} x_1 + \ldots + \frac{a_{m,n-1}}{a_{mn}} x_{n-1}\right).
      \end{equation*}
      Substituindo em cada uma das $m-1$ primeiras equações a incógnita $x_n$ por esta expressão, obtemos um sistema homogêneo de $m-1$ equações nas $n-1$ primeiras incógnitas.}}
  
  \only<5>{\footnotesize{Pela hipótese de indução, este sistema admite uma solução não-trivial $(\alpha_1,\ldots,\alpha_{n-1})$, pois $n-1>m-1$. Escrevendo então
      \begin{equation*}
        \alpha_n = -\left( \frac{a_{m1}}{a_{mn}} \alpha_1 + \ldots + \frac{a_{m,n-1}}{a_{mn}}\alpha_{n-1}\right),
      \end{equation*}
      obtemos uma solução não-trivial $(\alpha_1,\ldots,\alpha_{n-1},\alpha_n)$ do sistema proposto.}}
\end{frame}

\begin{frame}{}
  \only<1>{Usando os lemas anteriores, podemos provar o seguinte resultado.}

  \begin{block}{Teorema}
    Se o conjunto finito de vetores $\{v_1,\ldots,v_m\}$ gera o espaço vetorial $E$, então qualquer conjunto com mais de $m$ vetores em $E$ é l.d.
  \end{block}
  \vfill
  \only<2->{\alert{Demonstração.} }
  \only<3-4>{Dados os vetores $w_1,\ldots,w_n\in E$, com $n>m$, para cada $j=1,\ldots,n$ podemos escrever
    \begin{equation*}
      \zerodisplayskips
      w_j = \alpha_{1j}v_1 + \ldots + \alpha_{mj}v_m,
    \end{equation*}
    pois os vetores $v_j$ geram $E$.}
  
  \only<4>{Para mostrar que os vetores $w_j$ são l.d., devemos achar coeficientes $x_1,\ldots,x_n$, com pelo menos um deles não-nulo, tais que $x_1w_1+\ldots+x_nw_n=0$.}
  \only<5-6>{Substituindo os $w_j$ por suas expressões em termos de $v_j$ e reorganizando a soma, esta igualdade significa que
    \begin{equation*}
      \zerodisplayskips
      \left( \sum_{j=1}^n x_j \alpha_{1j} \right) v_1 + \left( \sum_{j=1}^n x_j \alpha_{2j} \right) v_2 + \ldots +\left( \sum_{j=1}^n x_j \alpha_{mj} \right) v_m =0.
    \end{equation*}}
  \only<6-7>{Esta condição será satisfeita se todos os somatórios forem nulos, ou seja,
    \begin{align*}
      \zerodisplayskips
      \alpha_{11}x_1 + \alpha_{12}x_2 + \ldots + \alpha_{1n}x_n &= 0\\
      \alpha_{21}x_1 + \alpha_{22}x_2 + \ldots + \alpha_{2n}x_n &= 0\\
      \vdots &  \\
      \alpha_{m1}x_1 + \alpha_{m2}x_2 + \ldots + \alpha_{mn}x_n &= 0\\
    \end{align*}}
  \only<7>{Mas tal solução existe pelo Lema anterior, pois $n>m$. Portanto, $w_j$ são l.d. e o teorema está provado.}
\end{frame}

\begin{frame}{}
  \begin{block}{Corolário}
    Se os vetores $v_1,\ldots,v_m$ geram o espaço vetorial $E$ e os vetores $u_1,\ldots,u_n$ são l.i., então $n\leq m$.
  \end{block}
\end{frame}

\begin{frame}{}
  \begin{block}{Corolário}
    Se o espaço vetorial $E$ admite uma base ${\mathcal{B}}=\{ u_1,\ldots,u_n\}$ com $n$ elementos, então qualquer outra base de $E$ possui também $n$ elementos.
  \end{block}
\end{frame}
%  \begin{proof}
% Seja ${\mathcal{B}}'$ outra base de $E$ com $m$ elementos. Como ${\mathcal{B}}'$ gera $E$ e ${\mathcal{B}}$ é l.i., temos que $n\leq m$, pelo Corolário \ref{cor:1}. Como ${\mathcal{B}}$ gera $E$ e ${\mathcal{B}}'$ é l.i., do mesmo Corolário segue-se que $m\leq n$. Logo, $m=n$.
% \end{proof}

\begin{frame}{Dimensão}
  \begin{block}{Definição}
    Diz-se que o espaço vetorial $E$ tem \emph{dimensão finita} quando admite uma base ${\mathcal{B}}$ com um número finito $n$ de elementos. Este número, que é o mesmo para todas as bases de $E$, chama-se \emph{dimensão} do espaço vetorial $E$, $n=$dim$(E)$. Por extensão, diz-se que o espaço vetorial $E=\{0\}$ tem dimensão zero.   
  \end{block}
\end{frame}

\begin{frame}{}
  \begin{block}{Corolário}
    Se a dimensão de $E$ é $n$, um conjunto com $n$ vetores gera $E$ se e somente se é l.i.
  \end{block}
% \begin{proof}
% Se $X=\{v_1,\ldots,v_n\}$ gera $E$ e não é l.i., então um dos seus elementos é combinação dos $n-1$ vetores restantes. Logo, estes $n-1$ vetores restantes formariam também um conjunto de geradores de $E$, o que contradiz o Teorema~\ref{teo:33}, pois todas as bases de $E$ devem ter $n$ vetores linearmente independentes. 

% Reciprocamente, suponha que $X$ seja l.i. Se $X$ não gerasse $E$, existiria um vetor $v\in E$ que não seria combinação linear dos elementos de $X$. Então o conjunto $\{ v_1,\ldots,v_n,v\}$ seria l.i., em contradição com o Teorema~\ref{teo:33}, pois uma base de $E$ com $n$ elementos gera todo o espaço $E$.
% \end{proof}
\end{frame}

\begin{frame}{Exemplos}
  \only<1>{%
    \begin{itemize}
    \item Em ${\mathbb{R}}^2$, duas possíveis bases são $\{(1,0), (0,1)\}$ e $\{(1,1),(0,1)\}$. Este espaço tem dimensão 2.
    \end{itemize}
  }
  \only<2>{%
    \begin{itemize}
    \item O espaço ${\mathbb{R}}^n$ tem dimensão $n$ (basta pensar na base canônica de cada espaço destes, para $n\in {\mathbb{N}}$).
    \end{itemize}
  }
  \only<3-4>{%
    \begin{itemize}
    \item O espaço ${\mathcal{M}}_{2\times 2}$ tem dimensão \only<4>{\alert{4}}
    \end{itemize}
  }
\end{frame}

\begin{frame}{}
  \begin{block}{Teorema}
    Qualquer conjunto l.i. de um espaço vetorial $E$ com dimensão finita pode ser completado para formar uma base.
  \end{block}
  \vfill
  \only<2->{\alert{Demonstração.} Seja $n = $dim$(E)$ e $v_1,\ldots, v_r$ um conjunto l.i.}

  \only<3->{Por um Corolário anterior, $r\leq n$.} \only<4->{ Se esse conjunto gera $E$, então ele é base e $n=r$. Suponha que não. }

  \only<5->{Então existe um $v_{r+1} \in E$ tal que $v_{r+1}\not\in \{v_1,\ldots,v_r\}$. Então $v_{r+1}$ não pode ser combinação linear dos $v_i$, pois caso contrário os $v_i$ seriam base para $E$. } \only<5->{Logo, $\{v_1,\ldots,v_r,v_{r+1}\}$ é l.i. Se este conjunto gera $E$, terminamos. Senão, continuamos no mesmo procedimento até que uma base tenha sido encontrada.}
\end{frame}

\begin{frame}{}
  \begin{block}{Corolário}
    Seja $E$ um espaço vetorial de dimensão finita. Se $U$ é subespaço de $E$, então 
    \begin{equation*}
      \text{dim}(U) \leq \text{dim}(E).
    \end{equation*}
  \end{block}
  \vfill
  \only<2>{\alert{Demonstração.} Para isto, basta pensarmos que uma base de $U$ deve estar contida em $E$, e assim não pode ter mais elementos do que uma base de $E$.}
\end{frame}

\begin{frame}{}
  \begin{block}{Corolário}
    Se $n=$dim$(V)$, com $V\subset W$ e dim$(W) = n$, então $V=W$.
  \end{block}
\end{frame}

\begin{frame}{Surpreendente?}
  \begin{block}{Corolário}
    Seja $A\in {\mathbb{R}}^{n\times n}$ e suponha que as linhas de $A$ formam um conjunto l.i. Então $A$ é inversível.
  \end{block}
  \vfill
  \begin{footnotesize}
  \only<2->{\alert{Demonstração. } Seja para cada $i=1,\ldots, n$ o vetor $\alpha_i = (a_{i1}, a_{i2},\ldots, a_{in})$ (a linha $i$ da matriz $A$). Suponha que $W$ é o subespaço gerado por estes vetores.}
  \only<3->{Como as linhas são linearmente independentes por hipótese, a dimensão de $W$ é $n$. } \only<4->{Pelo Corolário, $W={\mathbb{R}}^n$. Portanto, devem existir escalares $b_{ij} \in {\mathbb{R}}$, $1\leq i,j\leq n$, tais que 
    \begin{equation*}
      \zerodisplayskips
      \begin{cases}
        e_1 &= b_{11}\alpha_1+b_{12}\alpha_2+\ldots+b_{1n}\alpha_n\\
        e_2 &= b_{21}\alpha_1+b_{22}\alpha_2+\ldots+b_{2n}\alpha_n\\
        \vdots  & \\
        e_n &= b_{n1}\alpha_1+b_{n2}\alpha_2+\ldots+b_{nn}\alpha_n\\
      \end{cases}
    \end{equation*}
    em que cada $e_i$ é o i-ésimo vetor canônico em ${\mathbb{R}}^n$.}
  \only<4->{Portanto, se construirmos uma matriz com os $b_{ij}$ teremos que $BA = I$, ou seja, $B=A^{-1}$.}
  \end{footnotesize}
\end{frame}

\begin{frame}{Finalmente...}
  \begin{block}{Teorema}
    Se $U$ e $W$ são subespaços de $E$ (espaço vetorial com dimensão finita), então 
    \begin{equation*}
      \text{dim}(U+W) = \text{dim}(U) + \text{dim}(W) - \text{dim}(U\cap W)
    \end{equation*}
  \end{block}
\end{frame}

\begin{frame}{Demonstração}
  \begin{footnotesize}
    \only<1-2>{Note que $U\cap W$ e é subespaço de $E$. Portanto, deve ter dimensão finita e sua dimensão deve ser menor que a dimensão de $E$. Assim, sua base deve ser um subconjunto $\alpha_1,\ldots,\alpha_k$ da base de $U$ e da base de $W$. Escreva isso então como
      \begin{align*}
        U &= \text{span}[\alpha_1,\ldots,\alpha_k,\beta_{k+1},\ldots,\beta_m]\\
        W &= \text{span}[\alpha_1,\ldots,\alpha_k,\gamma_{k+1},\ldots,\gamma_n]
      \end{align*}}
    \only<2>{Assim, o subespaço $U+W$ é gerado pelos vetores $\{\alpha_i, \beta_j, \gamma_p\}$, com $1\leq i\leq k$, $k+1\leq j\leq m$, $k+1\leq p\leq n$.}
    \only<3->{O subespaço $U+W$ é gerado pelos vetores $\{\alpha_i, \beta_j, \gamma_p\}$, com $1\leq i\leq k$, $k+1\leq j\leq m$, $k+1\leq p\leq n$.}
    
    \only<4->{Agora, note que estes vetores formam um conjunto linearmente independente, pois caso contrário poderíamos escrever, por exemplo, $\sum x_i\alpha_i + \sum y_j\beta_j + \sum z_p\gamma_p = 0$ e assim teríamos 
      \begin{equation*}
        \zerodisplayskips
        -\sum y_j\beta_j = \sum x_i \alpha_i + \sum z_p \gamma_p
      \end{equation*}
      o que implicaria que $\beta_j \in W$.}
    
    \only<5->{Como os $\beta_j \in U$, isso implicaria que poderíamos escrever
      \begin{equation*}
        \zerodisplayskips
        \sum y_j \beta_j = \sum c_i\alpha_i
      \end{equation*}
      para escalares $c_i$. Mas, como a base de $U$ é linearmente independente, cada um dos escalares $y_j$ deve ser igual a zero; logo,
      \begin{equation*}
        \zerodisplayskips
        0 = -\sum y_j\beta_j = \sum x_i \alpha_i + \sum z_p \gamma_p
      \end{equation*}
      e como $\{\alpha_i,\gamma_q\}$ também é l.i., todos os $x_i$ e todos os $z_p$ devem ser iguais a zero. Portanto, $\{\alpha_i,\beta_j,\gamma_p\}$ formam uma base para $U+W$.}

    \only<6->{Além disso,
    \begin{equation*}
      \zerodisplayskips
      \text{dim}(U) + \text{dim}(W) = m+n = k+(m+n-k) = \text{dim}(U\cap W) + \text{dim}(U+W).
    \end{equation*}}
  \end{footnotesize}
\end{frame}

\begin{frame}{Exemplo}
  Seja $P$ o espaço dos polinômios em ${\mathbb{R}}$ (de qualquer grau).  Então os vetores deste espaço tem a forma
  \begin{equation*}
     f(x) = a_0+a_1x+\ldots +a_nx^n
  \end{equation*}
  Seja $f_k(x) = x^k$, $k=0,1,\ldots$. O conjunto infinito $\{f_0, f_1,\ldots\}$ é uma base para $V$. (Basta mostrarmos que todo subconjunto finito deste conjunto infinito é l.i.) Isto implica que ele não tem dimensão finita, pelo teorema anterior que dizia que um conjunto l.i. não pode ter mais elementos do que a dimensão do espaço. 
  
 \alert{Observação.} Uma base infinita não requer combinações lineares infinitas; $\sum_{k=0}^{\infty} a_kx^k$ não está neste espaço.
\end{frame}

\begin{frame}{Coordenadas}
  Para determinarmos as coordenadas de um vetor (isto é, os coeficientes da combinação linear dos elementos da base que o definem) precisamos fazer isso numa certa ordem.

  \alert{Exemplo:}
  
  $(2,3) = 2(1,0)+3(0,1)$.

  Mas se $\overline{e_1} = (0,1)$ e $\overline{e_2} = (1,0)$, então as coordenadas mudam para 
  \begin{equation*}
     (2,3) = 3\overline{e_1}+2\overline{e_2} = (3,2)_{\text{(nova base)}}
  \end{equation*}
\end{frame}

\begin{frame}{Base}
  \begin{block}{Definição}
    Se $E$ é um espaço vetorial de dimensão finita, uma base ordenada de $E$ é uma \emph{sequência} finita de vetores que é l.i. e que gera $E$.
  \end{block}
  \vfill
  \only<2>{Desta forma, dada uma base ordenada ${\mathcal{B}} = \{\alpha_1,\alpha_2,\ldots,\alpha_n\}$ de $V$, então dado $v\in V$, existe uma única $n$-tupla de escalares $x_i$ tais que
    \begin{equation*}
      v = \sum_{i=1}^n x_i \alpha_i.
    \end{equation*}}
\end{frame}

\begin{frame}{A base não é única}
  Frequentemente, ao invés de trabalharmos com as coordenadas de um vetor, vamos precisar trabalhar com a matriz de $v$ relativa à base ordenada ${\mathcal{B}}$:
  \begin{equation*}
    v =
    \begin{pmatrix}
      x_1\\\vdots\\ x_n
    \end{pmatrix}_{\mathcal{B}}
  \end{equation*}
  Isto é útil pois vamos tentar descrever o que acontece quando mudamos de base.
\end{frame}

\begin{frame}{Mudança de base}
  \begin{block}{Teorema}
    Seja $E$ um espaço vetorial de dimensão $n$ e sejam ${\mathcal{B}}$ e ${\mathcal{B}}'$ duas bases ordenadas de $E$. Então existe uma matriz única $P$, inversível e $n\times n$, com entradas tais que
    \begin{equation*}
      [v]_{\mathcal{B}} = P[v]_{{\mathcal{B}}'} \quad \mbox{ e } \quad [v]_{{\mathcal{B}}'}=P^{-1}[v]_{\mathcal{B}},
    \end{equation*}
    para todo $v\in E$. As colunas de $P$ são dadas por 
    \begin{equation*}
      P_j = [\alpha'_j]_{\mathcal{B}}, \quad j = 1,\ldots,n.
    \end{equation*}
  \end{block}
\end{frame}

\begin{frame}{Demonstração}
  \begin{footnotesize}
    \alert{Demonstração. } \only<1-2>{Considere as bases
      \begin{equation*}
        \zerodisplayskips
        {\mathcal{B}} = \{\alpha_1,\ldots,\alpha_n\} \mbox{ e } {\mathcal{B}}' = \{ \alpha_1',\ldots,\alpha_n'\}
      \end{equation*}
      Então, existe um conjunto único de escalares $P_{ij}$ tais que
      \begin{equation*}
        \zerodisplayskips
        \alpha_j' = \sum_{i=1}^n P_{ij} \alpha_i, 1\leq j\leq n.
      \end{equation*}
    }
    \only<2>{Sejam agora $x_1,\ldots,x_n$ as coordenadas de um vetor $v$ na base ordenada ${\mathcal{B}}$ e $x_1',\ldots,x_n'$ as coordenadas do mesmo vetor $v$ na base ordenada ${\mathcal{B}}'$. Então, \vspace*{-0.5cm}
      \begin{align*}
        \zerodisplayskips
        v &= x_1'\alpha_1'+\ldots + x_n'\alpha_n' = \sum_{j=1}^n x_j'\alpha_j'\\
          &= \sum_{j=1}^nx_j'\sum_{i=1}^n P_{ij}\alpha_i\\
          &= \sum_{j=1}^n\sum_{i=1}^n (P_{ij}x_j')\alpha_i\\
          &= \sum_{j=1}^n\left(\sum_{i=1}^n P_{ij}x_j'\right)\alpha_i
      \end{align*}}
    \only<3->{Como as coordenadas são unicamente determinadas para cada base, isso implica que
    \begin{equation*}
      x_i = \sum_{j=1}^n P_{ij} x_j', 1\leq i\leq n
    \end{equation*}
    Seja então $P$ a matriz formada pelos $P_{ij}$ e $X$ e $X'$ as matrizes coordenadas do vetor $v$ nas bases ${\mathcal{B}}$ e ${\mathcal{B}}'$, respectivamente. Então,
    \begin{equation*}
      X = PX'.
    \end{equation*}
    Como as duas bases são linearmente independentes, $X=0$ se e somente se $X'=0$. Logo, segue de um teorema anterior que $P$ é inversível; ou seja
    \begin{equation*}
      X' = P^{-1}X.
    \end{equation*}
    Em outras palavras,
    \begin{equation*}
      [v]_{\mathcal{B}} = P[v]_{{\mathcal{B}}'} \mbox{ e } [v]_{{\mathcal{B}}'} = P^{-1}[v]_{\mathcal{B}}
    \end{equation*}}
  \end{footnotesize}
\end{frame}

\begin{frame}{Mudança de base}
  Isto quer dizer que para construirmos $P$ que leva um vetor descrito na base ${\mathcal{B}}'$ em sua descrição na base ${\mathcal{B}}$, devemos escrever cada vetor da base ${\mathcal{B}}'$ em suas coordenadas na base $B$. Podemos denotar também 
 \begin{equation*}
 	P=I_{\mathcal{B}}^{{\mathcal{B}}'}.
 \end{equation*}
\end{frame}

\begin{frame}{}
  \begin{block}{Teorema}
    Seja $P$ uma matriz $n\times n$ inversível, e seja $V$ um espaço $n$-dimensional definido no mesmo corpo; além disso, seja ${\mathcal{B}}$ uma base ordenada de $V$. Então existe uma única base ordenada ${\mathcal{B}}'$ de $V$ tal que $P$ é a matriz de mudança de base de ${\mathcal{B}}'$ para ${\mathcal{B}}$, ou seja, 
   \begin{equation*}
      [v]_{\mathcal{B}} = P[v]_{{\mathcal{B}}'} \mbox{ e } [v]_{{\mathcal{B}}'} = P^{-1}[v]_{\mathcal{B}}
   \end{equation*}
   para qualquer vetor $v\in V$.
 \end{block}
 \vfill
 \alert{Demonstração. } \only<1>{Seja ${\mathcal{B}} = \{\alpha_1,\ldots,\alpha_n\}$. Se ${\mathcal{B}}' = \{ \alpha_1',\ldots,\alpha_n'\}$ for uma base ordenada de $V$ para a qual a primeira igualdade é válida, então devemos ter
\begin{equation*}
  \alpha_j' = \sum_{i=1}^n P_{ij} \alpha_i.
\end{equation*}
Logo, precisamos somente mostrar que estes vetores $\alpha_j'$ formam uma base de $V$.}

\only<2>{Mas:
  \begin{equation*}
    \zerodisplayskips
    \sum_j P^{-1}_{jk}\alpha_j' = \sum_j P^{-1}_{jk} \sum_i P_{ij}\alpha_i = \sum_j \sum_i P_{ij}P^{-1}_{jk} \alpha_i = \alpha_k
  \end{equation*}
  Logo, o subespaço gerado por ${\mathcal{B}}'$ contém ${\mathcal{B}}$ e é portanto igual a $V$. Logo, ${\mathcal{B}}'$ é base; assim, as duas afirmações são verdadeiras.}
\end{frame}

\begin{frame}{Exemplo}
  Seja ${\mathcal{B}} = \{(1,0),(0,1)\}$ e ${\mathcal{B}}' = \{(1,1), (1,0)\}$ bases de ${\mathbb{R}}^2$. Então 
  \begin{align*}
    (1,0) &= 0(1,1)+1(1,0)\\
    (0,1) &= 1(1,1)-1(1,0)
  \end{align*}
  Logo,
  \begin{equation*}
    \begin{pmatrix}
      v_1\\v_2
    \end{pmatrix}_{{\mathcal{B}}'} = 
    \begin{pmatrix}
      0 & 1\\
      1 & -1
    \end{pmatrix}
    \begin{pmatrix}
      v_1\\v_2
    \end{pmatrix}_{{\mathcal{B}}}
  \end{equation*}
  Assim, 
  \begin{equation*}
     \begin{pmatrix}
        2\\3
     \end{pmatrix}_{{\mathcal{B}}'} =
     \begin{pmatrix}
        0 & 1\\ 1 & -1
     \end{pmatrix} 
     \begin{pmatrix}
        2\\3
     \end{pmatrix}_{\mathcal{B}} = 
     \begin{pmatrix}
        3\\-1
     \end{pmatrix}_{\mathcal{B}}.
  \end{equation*}
\end{frame}

\begin{frame}{Exemplo}
  Considere ${\mathcal{B}} = \{(1,0),(0,1)\}$, ${\mathcal{B}}'=\{(2,3),(-1,2)\}$. Para construirmos $I_{\mathcal{B}}^{{\mathcal{B}}'}$, escrevemos cada vetor da base ${\mathcal{B}}'$ na base ${\mathcal{B}}$:\vspace*{-0.5cm}
  \begin{align*}
    (2,3) & = \only<2->{2(1,0)+3(0,1)}\\
    (-1,2) & = \only<2->{-1(1,0)+2(0,1)}
  \end{align*}
  \only<3->{Logo,
    \begin{equation*}
      \zerodisplayskips
      I_{\mathcal{B}}^{{\mathcal{B}}'} = \only<4->{\begin{pmatrix} 2 & -1\\3 & 2\end{pmatrix}}
    \end{equation*}}
  \only<5->{Note que, se $v=(1,1)_{{\mathcal{B}}'}$, então em ${\mathcal{B}}'$ teremos}
  \only<6->{%
    \begin{equation*}
      \zerodisplayskips
      v = 1(2,3)+1(-1,2) = (1,5)_{\mathcal{B}}.
    \end{equation*}}
  \only<7>{De fato:
  \begin{equation*}
    \zerodisplayskips
    I_{\mathcal{B}}^{{\mathcal{B}}'} v_{{\mathcal{B}}'} = \begin{pmatrix} 2 & -1\\3 & 2\end{pmatrix} \begin{pmatrix} 1\\1\end{pmatrix} = \begin{pmatrix} 1\\5\end{pmatrix}.
  \end{equation*}}
\end{frame}

\begin{frame}{Exemplo: continuação}
  Por outro lado, \vspace*{-0.5cm}
  \begin{align*}
    (1,0) & = \frac{2}{7}(2,3) - \frac{3}{7}(-1,2)\\
    (0,1) & = \frac{1}{7}(2,3) + \frac{2}{7}(-1,2)
  \end{align*}
  Assim,
  \begin{equation*}
    I_{{\mathcal{B}}'}^{\mathcal{B}} = \begin{pmatrix} \frac{2}{7} & \frac{1}{7}\\\-\frac{3}{7} & \frac{2}{7}\end{pmatrix}.
  \end{equation*}
  De fato,
  \begin{equation*}
    I_{{\mathcal{B}}'}^{\mathcal{B}}v_{\mathcal{B}} = \begin{pmatrix} \frac{2}{7} & \frac{1}{7}\\ -\frac{3}{7} & \frac{2}{7}\end{pmatrix} \begin{pmatrix} 1\\5\end{pmatrix} = \begin{pmatrix} 1\\1\end{pmatrix}_{\mathcal{B}}.
  \end{equation*}
  Ainda:
  \begin{equation*}
    I_{{\mathcal{B}}'}^{\mathcal{B}}I_{\mathcal{B}}^{{\mathcal{B}}'} = I.
  \end{equation*}
\end{frame}

\begin{frame}{Exemplo}
  Se ${\mathcal{B}} = \{ (1,2),(3,5)\}$ e ${\mathcal{B}}' = \{(1,-1),(1,-2)\}$, para encontrarmos $I_{\mathcal{B}}^{{\mathcal{B}}'}$ devemos escrever os elementos de ${\mathcal{B}}'$ na base ${\mathcal{B}}$. Mas isso pode ser difícil. Considere então a base canônica em ${\mathbb{R}}^2$. Então:
  \begin{equation*}
    I_C^{{\mathcal{B}}'} = \begin{pmatrix} 1 & 1\\-1 & -2\end{pmatrix}.
  \end{equation*}
  Além disso, \vspace*{-0.5cm}
  \begin{align*}
    (1,0) &= -5(1,2)+2(3,5)\\
    (0,1) &= 3(1,2)-1(3,5)
  \end{align*}
  \vspace*{-0.5cm} e assim
  \begin{equation*}
    I_{\mathcal{B}}^C = \begin{pmatrix} -5 & 3\\2 & -1\end{pmatrix}.
  \end{equation*}
\end{frame}

\begin{frame}{Exemplo: continuação}
  Então:
  \begin{equation*}
    I_{\mathcal{B}}^{{\mathcal{B}}'} = I_{\mathcal{B}}^C I_C^{{\mathcal{B}}'} = \begin{pmatrix} -5 & 3\\2 & -1\end{pmatrix} \begin{pmatrix} 1 & 1\\-1 & -2\end{pmatrix} = \begin{pmatrix} -8 & -11 \\3 & 4\end{pmatrix}.
  \end{equation*}
  Note que
  \begin{equation*}
    I_{\mathcal{B}}^{{\mathcal{B}}'} v_{{\mathcal{B}}'} = \begin{pmatrix} -8 & -11\\3 & 4\end{pmatrix} \begin{pmatrix} 1\\1\end{pmatrix}_{{\mathcal{B}}'} = \begin{pmatrix} -19\\7\end{pmatrix}
  \end{equation*}
  De fato,
  \begin{align*}
    (1,1)_{{\mathcal{B}}'} &= (1,-1)+(1,-2) = (2,3)_C\\
    (-19,7)_{\mathcal{B}} &= -19(1,2)+7(3,5) = (2,-3)_C.
  \end{align*}
\end{frame}

\begin{frame}{Exemplo}
  Em ${\mathbb{R}}^3$, se consideramos as bases\vspace*{-0.5cm}
  \begin{align*}
    E &= \{ (1,0,0),(0,1,0),(0,0,1)\}\\
    S &= \{ (1,0,1),(2,1,2),(1,2,2)\}
  \end{align*}
  \vspace*{-0.5cm}temos que
  \begin{equation*}
    I_E^S = \begin{pmatrix} 1 & 2 & 1\\0 & 1 & 2\\1& 2 & 2\end{pmatrix} \qquad \text{ e } \qquad I_S^E = \begin{pmatrix} -2 & -2 & 3\\2 & 1 & -2\\1 & 0 & 1\end{pmatrix}
  \end{equation*}
  Assim, se $v = (1,1,1)_E$, temos
  \begin{equation*}
    I_S^E v_E = \begin{pmatrix} -2 & -2 & 3\\2 & 1 & 2\\1 & 0 & 1\end{pmatrix} \begin{pmatrix} 1\\1\\1\end{pmatrix}_E = \begin{pmatrix} -1\\1\\0\end{pmatrix}_S.
  \end{equation*}
\end{frame}

\begin{frame}{Exemplo}
  \begin{footnotesize}
  Seja $\theta \in {\mathbb{R}}$; a matriz 
  \begin{equation*}
     P = 
     \begin{pmatrix}
        \cos{\theta} & -\sin{\theta}\\
        \sin{\theta} & \cos{\theta}
     \end{pmatrix}
  \end{equation*}
  é inversível com inversa
  \begin{equation*}
     \begin{pmatrix}
        \cos{\theta} & \sin{\theta}\\
        -\sin{\theta} & \cos{\theta}
     \end{pmatrix}
  \end{equation*}
  Logo, para cada $\theta$, o conjunto ${\mathcal{B}}'$ formado pelos vetores $(\cos{\theta}, \sin{\theta})$, $(-\sin{\theta}, \cos{\theta})$ é uma base de ${\mathbb{R}}^2$. Intuitivamente esta base é obtida ao rotacionarmos a base canônica num ângulo $\theta$. Se $\alpha =(x_1,x_2)$, então
  \begin{equation*}
     [\alpha]_{{\mathcal{B}}'} = 
     \begin{pmatrix}
        \cos{\theta} & \sin{\theta}\\
        -\sin{\theta} &\cos{\theta}
     \end{pmatrix}
     \begin{pmatrix}
        x_1\\x_2
     \end{pmatrix}
  \end{equation*}
  ou ainda
  \begin{align*}
    x_1' &= x_1\cos{\theta}+x_2\sin{\theta}\\
    x_2' &= -x_1\sin{\theta}+x_2\cos{\theta}      
  \end{align*}
\end{footnotesize}
\end{frame}

\end{darkframes}
\end{document}

